from google.colab import drive
drive.mount('/content/drive')

!pip install llama-cpp-python faiss-cpu langchain sentence-transformers pymupdf


import os
model_url = "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
model_path = "/content/mistral-7b.Q4_K_M.gguf"
if not os.path.exists(model_path):
    !wget -O /content/mistral-7b.Q4_K_M.gguf $model_url


import fitz

filename = "/content/drive/MyDrive/rag/2020.nlpcovid19-acl.1.pdf"
def extract_text_from_pdf(filepath):
    doc = fitz.open(filepath)
    text_chunks = []
    for page in doc:
        text = page.get_text()
        if text.strip():
            text_chunks.append(text.strip())
    return text_chunks

text_chunks = extract_text_from_pdf(filename)
print(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø­ØªÙˆÙŠØ© Ø¹Ù„Ù‰ Ù†ØµÙˆØµ: {len(text_chunks)}")


!pip install -U langchain-community

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
documents = [Document(page_content=text) for text in text_chunks]
db = FAISS.from_documents(documents, embedding_model)


from llama_cpp import Llama

llm = Llama(
    model_path="/content/mistral-7b.Q4_K_M.gguf",
    n_ctx=4096,
    n_threads=4,
    n_gpu_layers=20
)

def answer_with_mistral(question, context):
    prompt = f"""You are an expert assistant. Use the following context to answer the question.

Context:
{context}

Question: {question}
Answer:"""
    result = llm(prompt, max_tokens=300, stop=["User:", "###"])
    return result["choices"][0]["text"].strip()

question = "What is cord19?"
docs = db.similarity_search(question, k=3)
context = "\n".join([doc.page_content for doc in docs])

answer = answer_with_mistral(question, context)
print("Answer:", answer)


!pip install gradio -q


import gradio as gr

def ask_question(user_input):
    docs = db.similarity_search(user_input, k=3)
    context = "\n".join([doc.page_content for doc in docs])
    return answer_with_mistral(user_input, context)

interface = gr.Interface(
    fn=ask_question,
    inputs=gr.Textbox(lines=2, placeholder="Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§ Ø¹Ù† Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù€ PDF..."),
    outputs="text",
    title="ğŸ’¬ Ù…Ø³Ø§Ø¹Ø¯ PDF Ø§Ù„Ø°ÙƒÙŠ - Mistral RAG",
    description="Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ù‹Ø§ Ø­ÙˆÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¨Ø­Ø« ÙˆØ³Ø£Ø¬ÙŠØ¨Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Mistral ÙˆØ§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ."
)

interface.launch(share=True)

